# Workana Scraping
## üéØ Objetivo 
Extrair dados de trabalhos ofertados a profissionais freelancers na plataforma Workana.

De cada trabalho √© preciso coletar:
- [x] T√≠tulo
- [x] Skills
- [x] Propostas
- [x] Resumo
- [x] Data
- [x] Budget
- [x] Links
- [x] Categorias
- [x] Subcategoria

## üõ†Ô∏è Tools
- bash
- Vscode
- Python 3.11.2
- Poetry 1.4.0
- Jupyter Notebook
- Selenium 4.9.0
- Pandas 2.0.1

> [Leia a Documenta√ß√£o](references.md)

# Extra√ß√£o de Dados com Selenium

> [Leia a Documenta√ß√£o do Selenium](https://www.selenium.dev/documentation/)

Neste texto mostrarei como fiz para coletar os dados de trabalhos ofertados na plataforma Workana. Para verificar o caminho at√© este ponto do projeto consulte: 
- [Scrap one job](../notebooks/scrap_workana_one_job.ipynb) 
- [Scrap one page](../notebooks/scrap_workana_one_page.ipynb)

Enquanto o notebook base para este texto √©: 
- [Scrap all jobs](../notebooks/scrap_workana_all_jobs.ipynb)

## Instala√ß√£o
Antes de come√ßar, √© preciso instalar os pacotes que fazem o projeto funcionar. Voc√™ pode fazer isso usando o gerenciador de pacotes pip ou poetry:

```bash
pip install -r requirements.txt
```

```bash
poetry install
```

## Configura√ß√£o
Por padr√£o, para utilizar o Selenium √© preciso um driver espec√≠fico para cada navegador que desejamos utilizar. O driver deve ser baixado e adicionado ao PATH do sistema.

Por√©m essa etapa pode ser pulada se voc√™ utilizar a bibliotca ***Webdriver Manager*** que inicia sozinha uma sess√£o do navegador quando preciso.

Assim, podemos importar as bibliotecas,  instanciar e configurar uma nova sess√£o para iniciar a coleta de dados.

```python
# import libs
import pandas as pd
import time

from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

# init session
driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

# maximizing browser window
driver.maximize_window()
```

## Coletando os dados

De cada trabalho [o objetivo √©.](#objetivo)

### URL base

https://www.workana.com/jobs?language=pt&page=1

Observando a ***URL_BASE*** pelo driver pude perceber que os trabalhos s√£o apresentados em conjuntos de 9 trabalhos por p√°gina dentro do universo de 50 p√°ginas.

Deste modo foi criado um la√ßo que percorre as p√°ginas, iniciando em 1 at√© 50. Foi adicionado o tempo de espera entre p√°ginas de 3 segundos para ter certeza que tudo foi carregado dentro da plataforma antes de coletar os dados.

```python
for p in range(1,51):

    # go to jobs
    driver.get(f'https://www.workana.com/jobs?language=pt&page={p}')
    
    print(f'https://www.workana.com/jobs?language=pt&page={p}')
    
    # wait
    time.sleep(3)
```

### Onde est√£o as informa√ß√µe
#### **class="project-header"**

Nos elementos filhos da `<div class="project-header"></div>` podemos encontrar data, link e t√≠tulo dos trabalhos.

#### **class="project-body"**

Nos elementos filhos da `<div class="project-body"></div>` podemos encontrar data e o tempo desde a publica√ß√£o, n√∫mero de propostas, texto de resumo(as vezes texto todo), skills com seus links de refer√™ncia.

Dentro de `<span class="expander-details"></span>"` pode ser encontrado categoria, subcategoria, 'alcance do Projeto', '√© projeto ou trabalho', 'tenho atualmente', 'disponibilidade', 'fun√ß√µes necess√°rias' e outras informa√ß√µes.


#### **class="project-actions floating"**

Nos elementos filhos da `<div class="project-actions floating"></div>` podemos encontrar budget, o valor oferecido como or√ßamento.


### O caminho XPATH
√â poss√≠vel pegar as informa√ß√µes dos trabalhos com o XPATH b√°sico `//*[@id="projects"]/div[{i}]/`

>[Script falhando nas primeiras vezes](doubts.md#script-falhando-nas-primeiras-vezes)

Onde ***i*** √© maior ou igual a 2 e menor ou igual a 10, valores referentes as quantidades de jobs exibidos em cada p√°gina.

#### Data
```python
job_date = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{2}]/div[1]/h5')
job_date = job_date.get_attribute('title')
print('Publica√ß√£o:',job_date)
```

#### Link
```python
job_link = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{2}]/div[1]/h2/a')
job_link = job_link.get_attribute('href')
print(job_link)
```

#### T√≠tulo
```python
job_title = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{2}]/div[1]/h2/a/span')
job_title = job_title.get_attribute('title')
print('Tutilo:',job_title)
```

#### Qtd de Propostas Feitas
```python
job_bids = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{2}]/div[2]/div[1]/span[2]')
job_bids = job_bids.text.split(' ')[1]
print('Propostas:',job_bids)
```

#### Resumo
Algumas publica√ß√µes possuem um link que expande as informa√ß√µes exibidas na tela e para podermos capturar as informa√ß√µes de categoria e subcategoria no futuro necessitamos extrair todo o texto contido no resumo.
```python
if see_more.is_displayed():
    try:
        see_more.click()

        job_desc = driver.find_element(By.XPATH,f'//*[@id="projects"]/div[{i}]/div[2]/div[2]/div')
        job_desc_more = driver.find_element(By.XPATH,f'//*[@id="projects"]/div[{i}]/div[2]/div[2]/div/span[2]')
        job_desc = job_desc.text + job_desc_more.text
    except:
        print(f"error 'see more' in p:{p} i:{i}")
else:
    job_desc = driver.find_element(By.XPATH,f'//*[@id="projects"]/div[{i}]/div[2]/div[2]/div')
    job_desc = job_desc.text
```

#### Skills
Em alguns momentos a div que armazena as Skills n√£o aparece. Deste  modo √© preciso informar um valor padr√£o para ser inserido nos dados quando isto ocorrer.
```python
# get job skills 
    job_sk = []

    try:
        skills = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{i}]/div[2]/div[3]/div')
        skills = skills.find_elements(By.TAG_NAME, 'a')
    
        for skill in skills:
            job_sk.append(skill.text)

        job_sk = ', '.join(job_sk)
    except NoSuchElementException:
        job_sk.append('N√£o informado')
```

#### Budget do job
```python
job_budget = driver.find_element(By.XPATH, f'//*[@id="projects"]/div[{2}]/div[3]/h4/span')
job_budget = job_budget.text
print('Valor:',job_budget)
```

## Construindo dataframe com os dados coletados

Usaremos Pandas para transformar as informa√ß√µes extraidos do Workana em um dataframe a fim de facilitar a manipula√ß√£o dos dados. Para isso √© preciso transformar as informa√ß√µes coletadas em uma lista que ser√° lida pelo m√©todo da biblioteca Pandas que cria um dataframe.

> [Leia a documenta√ß√£o do Pandas](https://pandas.pydata.org/docs/)

### Transformando os dados para caber em um dataframe
Criei um dicion√°rio que recebe os valores extraidos da plataforma e acrescentei a chave 'Update' com valor 'False' para poder testar a atualiza√ß√£o dos dados.
```python
data_temp = {'Job': job_title,'Publish Date': job_date, 'Skills': job_sk,  'Budget':job_budget, 
                'Bids': job_bids, 'Summary': job_desc, 'Link': job_link, "Updated": False,}
```

### Criando um dataframe e exportando os dados para um CSV
Nosso script roda em loop infinito, para que os dados n√£o sejam inseridos repetidas vezes em nosso arquivo CSV criei uma verifica√ß√£o pelo link. Caso o link n√£o exista no arquivo CSV os dados ser√£o inseridos diretamente no arquivo. Entretanto, se o link j√° estiver presente as outras informa√ß√µes ser√£o atualizadas assim como a colua 'Update' que agora passa a valer 'True'.
```python
# #CSV path
csv_path = "workana_scraping/data/data_raw.csv"

# CSV exists
file_exists = False

if os.path.exists(csv_path):
    file_exists = True

    # link is in CSV
    link_exists = False

    if file_exists:
        df = pd.read_csv(csv_path)
        if df["Link"].eq(data_tmp["Link"]).any():
            link_exists = True
            # update row
            df.loc[df["Link"] == data_tmp["Link"],["Job","Publish Date","Skills","Budget","Bids","Summary","Updated"]] = [data_tmp["Job"],data_tmp["Publish Date"],data_tmp["Skills"],data_tmp["Budget"],data_tmp["Bids"],data_tmp["Summary"],True]

            # save update
            df.to_csv(csv_path, index=False)

    # append newline if file does not exist or if link is not present in CSV
    if not file_exists or not link_exists:
        df_nova_linha = pd.DataFrame([data_tmp])
        df_nova_linha.to_csv(csv_path, mode="a", index=False, header=not file_exists)
```

## Conclus√£o
Neste ponto j√° √© poss√≠vel ter acesso as primeiras informa√ß√µes coletadas do Workana em um dataframe. Ap√≥s esta etapa realizo alguns filtros para que a cada loop realizado algumas informa√ß√µes sejam enviadas ao bot do telegram. [Verifique a constru√ß√£o do bot](bot_telegram.md)

Com todas as etapas concluidas √© preciso tratar os dados para que seja poss√≠vel utiliz√°-los em algor√≠timos estat√≠sticos.
[Continue para a sess√£o de tratamento de dados](transform_data.md).
